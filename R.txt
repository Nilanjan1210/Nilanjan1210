#                       Practical 01
# 1. Generate sufficient random numbers from exponential distribution where the parameter is of your choice. Let consider these generated numbers as a lifetime of the patients then obtain the patient lifetime using the following censoring schemes:

# Right Censoring
# Type-I Censoring Scheme
# Type-II Censoring Scheme
# Random Censoring
# Left Censoring
# Interval Censoring

# Also compare the censored data with the complete data.

set.seed(29)
rate <- 0.1
lifetimes <- rexp(100, rate)
lifetimes
summary(lifetimes)

# Type 1 censoring
ctime <- 15
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
type1.data <- cbind(cdata, cstuts)
print(type1.data)


# Type 2 Censoring
ctime = sort(lifetimes)[80] 
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
type2.data <- cbind(cdata, cstuts)
print(type2.data)


# Random Censoring
ctime <- rexp(100, rate)
ctime
cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
cstuts <- ifelse(lifetimes<=ctime,1, 0)
random.data <- cbind(cdata, cstuts)
print(random.data)

# Left Censoring 
ctime <- 3
ctime
cdata <- ifelse(lifetimes<=ctime, ctime, lifetimes)
cstuts <- ifelse(lifetimes<=ctime, 0,1)
left.data <- cbind(cdata, cstuts)
print(left.data)

# Interval Censoring

intervals <- seq(0,60, 6)
intervals
freq <- table(cut(lifetimes, breaks = intervals))
print(freq)

#############################################################################
#                            Practical 02
# Obtain a sufficiently large sample from an exponential distribution under a Type II
# censoring scheme. After generating the sample, calculate the maximum likelihood
# (ML) estimate of the distribution parameter. Also, evaluate the performance of the
# estimate by computing the bias, variance, and mean squared error (MSE) for different
# sample sizes.



table <- data.frame(
  no_obs = c(),
  no_cen_obs = c(),
  lamda = c(),
  lamda_hat = c(),
  bias = c(),
  variance = c(),
  MSE = c()
)
lamda = 0.1
no_of_cen_obs = c(10,20,30,40,50,60,70,80,90,100)

for( k in no_of_cen_obs){
lamda_tem <- numeric(100)
for(i in 1:100){
  set.seed(i+1)
  lifetimes <- rexp(100, lamda)
  ctime = sort(lifetimes)[k] 
  cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
  lamda_tem[i] = k/sum(cdata)
}
var = var(lamda_tem)
mse = sum((lamda_tem- lamda)^2)/100
lamda_hat= mean(lamda_tem)
bias = lamda_hat - lamda
new_row = data.frame(no_obs = 100,
              no_cen_obs = k,
              lamda = lamda,
              lamda_hat = lamda_hat,
              bias = bias,
              variance = var,
              MSE = mse)
table = rbind(table, new_row)

}
table



#####################################################################################
#                               Practical 03
# Obtain a sufficiently large sample from a Weibull distribution under a Type I
# censoring scheme. After generating the sample, calculate the maximum likelihood
# (ML) estimate of the distribution parameters. Also, evaluate the performance of the
# estimate by computing the bias, variance, and mean squared error (MSE) for different
# sample sizes.

table <- data.frame(
  no_obs = c(),
  no_cen_obs = c(),
  lamda = c(),
  lamda_hat = c(),
  bias = c(),
  variance = c(),
  MSE = c()
)
shape = 10
scale = 5
times = c(20, 25, 30, 35)


for( k in times){
  lamda_tem <- numeric(100)
  for(i in 1:100){
    set.seed(i+1)
    lifetimes <- rweibull(100, shape, scale = 1)
    ctime = sort(lifetimes)[k] 
    cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
    lamda_tem[i] = k/sum(cdata)
  }
  var = var(lamda_tem)
  mse = sum((lamda_tem- lamda)^2)/100
  lamda_hat= mean(lamda_tem)
  bias = lamda_hat - lamda
  new_row = data.frame(no_obs = 100,
                       no_cen_obs = k,
                       lamda = lamda,
                       lamda_hat = lamda_hat,
                       bias = bias,
                       variance = var,
                       MSE = mse)
  table = rbind(table, new_row)
  
}
table

# Parameters
set.seed(123)
n_values <- c(100, 200, 300)       # Sample sizes
true_shape <- 2                    # True Weibull shape parameter
true_scale <- 3                    # True Weibull scale parameter
censoring_time <- 5                # Censoring time for Type-I censoring
simulation <- 1000                 # Number of simulations

# Log-likelihood function for censored Weibull data
weibull_log_likelihood <- function(params, data, censoring_time) {
  shape <- params[1]; scale <- params[2]
  uncensored <- data[data <= censoring_time]
  censored <- data[data > censoring_time]
  
  # Log-likelihood: uncensored and censored contributions
  ll_uncensored <- sum(dweibull(uncensored, shape, scale, log = TRUE))
  ll_censored <- sum(pweibull(censored, shape, scale, lower.tail = FALSE, log.p = TRUE))
  
  return(-(ll_uncensored + ll_censored))  # Negative log-likelihood
}

# Performance Evaluation for Type-I Censoring
evaluate_performance <- function(n, shape, scale, censoring_time, simulation) {
  shape_estimates <- scale_estimates <- numeric(simulation)
  
  for (i in 1:simulation) {
    # Generate and censor data
    life <- rweibull(n, shape, scale)
    censored_life <- pmin(life, censoring_time)
    
    # Estimate parameters via MLE
    fit <- optim(par = c(1, 1), fn = weibull_log_likelihood, data = censored_life,
                 censoring_time = censoring_time, method = "L-BFGS-B", lower = c(0.1, 0.1))
    shape_estimates[i] <- fit$par[1]
    scale_estimates[i] <- fit$par[2]
  }
  
  # Calculate bias, variance, MSE
  return(list(
    bias_shape = mean(shape_estimates) - shape, 
    bias_scale = mean(scale_estimates) - scale,
    var_shape = var(shape_estimates), 
    var_scale = var(scale_estimates),
    mse_shape = mean((shape_estimates - shape)^2),
    mse_scale = mean((scale_estimates - scale)^2)
  ))
}

# Run simulations and collect results
results <- data.frame()
for (n in n_values) {
  perf <- evaluate_performance(n, true_shape, true_scale, censoring_time, simulation)
  results <- rbind(results, data.frame(
    n = n, shape_hat = true_shape - perf$bias_shape, scale_hat = true_scale - perf$bias_scale,
    bias_shape = perf$bias_shape, bias_scale = perf$bias_scale, 
    var_shape = perf$var_shape, var_scale = perf$var_scale,
    mse_shape = perf$mse_shape, mse_scale = perf$mse_scale
  ))
}

print(results)










#######################################################################################
#                             Practical 04
# Obtain a sufficiently large sample from a Weibull distribution under type II censoring
# scheme. After generating the sample, calculate the maximum likelihood (ML)
# estimate of the distribution parameters. Also, evaluate the performance of the estimate
# by computing the bias, variance, and mean squared error (MSE) for different sample
# sizes.



table <- data.frame(
  no_obs = c(),
  no_cen_obs = c(),
  scale = c(),
  scale_hat= c(),
  bias_scale = c(),
  variance_scale  = c(),
  MSE_scale  = c(),
  shape = c(),
  shape_hat = c(),
  bias_shape = c(),
  variance_shape = c(),
  MSE_shape = c()
)

shape = 6
scale = 3

lifetimes <- rweibull(100, shape = shape,scale = scale)
summary(lifetimes)

logLikWeib <- function(par, tt, status) {
  mu <- par[1]
  sigma <- par[2]
  lambda.p <- exp(-mu)
  alpha.p <- 1/sigma
  dd <- sum(status)
  sum.t <- sum(status*log(tt))
  sum.t.alpha <- sum(tt^alpha.p)
  term.1 <- dd*log(alpha.p) + alpha.p*dd*log(lambda.p)
  term.2 <- (alpha.p- 1)*sum.t
  term.3 <- (lambda.p^alpha.p)*sum.t.alpha
  result <- term.1 + term.2- term.3
  result
}
no_of_cen_obs = c(10)


for( k in no_of_cen_obs){
  temp_mu <- c()
  temp_sigma <- c()
  for(i in 1:10){
    lifetimes <- rweibull(1000, shape = shape,scale = scale)
    ctime = sort(lifetimes)[k]
    cdata <- ifelse(lifetimes<=ctime,lifetimes, ctime)
    cstuts <- ifelse(lifetimes<=ctime,1, 0)
    result = result <- optim(par=c(4.568, 2.280), fn=logLikWeib, method=
                               "L-BFGS-B",
                             lower=c(1, 1), upper=c(10, 10),
                             control=list(fnscale =-1),
                             tt=cdata, status=cstuts)
    print(result)
    if(result$convergence == 0){
      temp_mu = c(temp_mu, result$par[1])
      temp_sigma =c(temp_sigma, result$par[2])
    }
  }
  var1 = var(temp_mu)
  var2 = var(temp_sigma)
  mse1 = sum((temp_mu- shape)^2)/100
  mse2 = sum((temp_sigma - scale)^2)/100
  mu = mean(temp_mu)
  sigma = mean(temp_sigma)
  bias1 = shape - mu
  bias2 = scale - sigma
  new_row = data.frame(  no_obs = 100,
                         no_cen_obs = k,
                         shape = shape,
                         shape_hat = mu,
                         bias_shape = bias1,
                         variance_shape = var1,
                         MSE_shape = mse1,
                         scale = scale,
                         scale_hat= sigma,
                         bias_scale =bias2 ,
                         variance_scale  = var2,
                         MSE_scale  = mse2
                         )
  table = rbind(table, new_row)
  
}
table








#######################################################################################
#                             Practical 05
# The following data represents the readmission times(weeks) for 21 people of
# leukemia patients. Group 1 is the treatment group and group 2 is the placebo group

# Group 1(treatment): 6,6,6,7,10,13,16,22,23,6+,9+,10+,11+,17+,19+,20+,25+,32+,32+,34+,35+

# Group 2(placebo) : 1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23

# Note: + denotes censored
# Estimate the survival and hazard curve for both group by the Kaplan-Meier method.

G1 <- c(6, 6, 6, 7, 10, 13, 16, 22, 23, 6, 9, 10, 11, 17, 19, 20, 25, 32, 32, 34, 35)  
G1_event <- c(rep(1,9), rep(0, 12))
G1_data <- data.frame(time = G1, event = G1_event)

G2 <- c(1, 1, 2, 2, 3, 4, 4, 5, 5, 8, 8, 8, 8, 11, 11, 12, 12, 15, 17, 22, 23)
G2_event <- c(rep(1,21))
G2_data <- data.frame(time = G2, event = G2_event)
# Sort the data 
#G1_data = G1_data[order(G1_data$time),]
#G2_data = G2_data[order(G2_data$time),]


km_estimator <- function(data){
  data = data[order(data$time),]
  n <- length(data$time)
  data$r <- 1:n
  for(i in 1:n){
    if(data$event[i] == 1){
      data$prob[i] <- (n-data$r[i])/(n-data$r[i]+1) 
    }
    else{
      data$prob[i] =0
    }
  }
  data1 <- data[data$event==1,]
  data1$sur <- cumprod(data1$prob)
  data1$cum_harz <- -log(data1$sur)
  return(data1)
}

km1 <- km_estimator(G1_data)
km1
km2 <- km_estimator(G2_data)
km2
plot(km1$time, km1$sur, type= 's', xlim = c(0,35), ylim = c(0,1), ylab = 'Probability', 
      xlab = "Time")
lines(km2$time, km2$sur, type= 's', col = "blue")

plot(km1$time, km1$cum_harz, type= 's', xlim = c(0,35), ylim = c(0,1), ylab = 'Probability', 
     xlab = "Time")
lines(km2$time, km2$cum_harz, type= 's', col = "blue")


#######################################################################################
#                             Practical 06
# A clinical trial investigates the effectiveness of a new drug on 10 lung cancer patients.
# The study started in January 2011 and continued till December 2012. The data is
# divided into two groups. Group A (treated with the new drug) and group B (treated with
# the standard treatment). The survival time is given in months. During the trial, some
# people joined. The data looks like below:
data <- data.frame(
  patient_id = 1:10,
  Group = c(rep("A",5),rep("B",5)),
  survival_time = c(5,12,18,20,24,8,10,15,16,22),
  censored = c(1,1,0,0,1,1,0,1,1,1)
)
View(data)
# Calculate and plot the PL estimate of the probability of surviving 2 years or more for
# both groups. Also, comment on the plots.

# b) Suppose that 10 patients joined at the beginning of 24 months; during those months
# 4 patients died and 6 patients survived. Estimate the proportion of patients in the
# population surviving for 24 months or more if the study terminates at 24 months.














#######################################################################################
#                             Practical 07
# 1. Generate sufficient random numbers from a normal distribution where the parameters
# are of your choices. Construct a normal probability paper and show that the generated
# sample gives the evidence to belong to the normal distribution. Also, comment on
# your result.

# 2. Generate sufficient random numbers from a distribution as per your roll number. Let's
# consider these generated numbers as the failure time of machines. First construct a
# probability paper for fitting purpose and then verify that the generated data well fits on
# the probability paper. Also, comment on your result.

# S.No. Model Name    Exam Roll No.
# 1.    Exponential   01-15
# 2.    Gamma         16-30
# 3.    Weibull       31-45
# 4.    Lognormal     45-60

### Noraml Distribution
set.seed(29)
n <- 1000
mean <- 10
sd <- 5
sample <- rnorm(n, mean, sd)
print(sample)

q1 <- qnorm(seq(0.01,0.99,0.05))
q2 <- qnorm(seq(0.01,0.99,0.05))
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)

sq <- sort(sample)
tq <- qnorm(seq(0.01, 0.99, length= n))
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')

### Exponential
set.seed(29)
n <- 100
rate = 0.01
sample <- rexp(n, rate)
print(sample)

q1 <- qexp(seq(0.01,0.99,0.05), rate = 0.01)
q2 <- qexp(seq(0.01,0.99,0.05), rate =  0.01)
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)
sq <- sort(sample)
tq <- qexp(seq(0.01, 0.99, length= n), rate = 0.01)
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')


###  Gamma 

set.seed(29)
n <- 100
shape = 10
sample <- rgamma(n, shape)
print(sample)

q1 <- qgamma(seq(0.01,0.99,0.05), shape = 10)
q2 <- qgamma(seq(0.01,0.99,0.05), shape =  10)
plot(q1,q2, type ='n', xlab = "Sample Quantiles", ylab = 'Theoretical Quantiles')
abline(v= q1)
abline(h= q1)
sq <- sort(sample)
tq <- qgamma(seq(0.01, 0.99, length= n), shape =10)
points(sq, tq, col =2, pch= 18)
abline(a=0 , b=1, col = 'blue')

### Weibull 


### Lognormal 


#######################################################################################
#                             Practical 08
data <- data.frame(patient_ID = 1:30,
                   failure_time = c(16.5,11.7,25.3,7.8,19.2,10.6,22.7,5.1,13.9,24.6,17.3,
                                    8.4,28.2,6.7,20.5,15.4,9.9,18.7,30.6,12.3,21.1,4.9,
                                    13.5,16.1,6.2,19.8,27.4,14.7,23.9,26.5))
# Suppose that the above data has been extracted from an experiment. The data represents
# the lifetime of patients. First, make a probability plot for the data and verify the
# distribution from which the data has been generated, and then estimate the parameter
# by using a graphical method.










#######################################################################################
#                             Practical 09

# The remission times of 42 patients with acute leukemia were reported in a clinical trial
# to assess the ability of 6-mercaptopurine(6-MP) to maintain remission. Patients were
# randomly selected to receive 6-MP or placebo. The study was terminated after one year.
# The following remission times, in weeks, were recorded:

#6-MP (21 patients) : 6,6,6,7,10,13,16,22,23,6+,9+,10+,11+,17+,19+,20+,25+,32+,32+,34+,35+
# Placebo (21 patients) : 1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23

# a) Now, fit a distribution to the remission duration of 6-MP patients using the hazard
# plotting technique.
# b) Estimate the parameter/parameters of the distribution.

# Data: Remission times for 6-MP group
remission_times_6MP <- c(6, 6, 6, 7, 10, 13, 16, 22, 23, 6, 9, 10, 11, 17, 19, 20, 25, 32, 32, 34, 35)
status_6MP <- c(rep(1, 9), rep(0, 12))  # 1 = observed, 0 = censored

# Sort remission times in increasing order
sorted_times <- sort(remission_times_6MP)

# Hazard plotting: Create ranks for plotting
ranks <- rank(sorted_times)

# Calculate the cumulative hazard H(t)
n <- length(sorted_times)
cum_hazard <- (ranks - 0.5) / n

# Plot the empirical hazard function (log-log plot)
plot(log(sorted_times), log(-log(1 - cum_hazard)), 
     xlab = "log(Time (weeks))", ylab = "log(-log(1 - F(t)))", 
     main = "Hazard Plot for 6-MP Patients", pch = 16)


plot(sorted_times, -log(1 - cum_hazard), 
     xlab = "log(Time (weeks))", ylab = "log(-log(1 - F(t)))", 
     main = "Hazard Plot for 6-MP Patients", pch = 16)


# Estimate Weibull parameters (Shape and Scale)
# Linear regression on log-log transformed data
X <- log(sorted_times)
Y <- log(-log(1 - cum_hazard))

# Linear regression (manually without using lm() function)
n <- length(X)
x_mean <- mean(X)
y_mean <- mean(Y)

# Calculate slope (beta) and intercept (alpha)
beta <- sum((X - x_mean) * (Y - y_mean)) / sum((X - x_mean)^2)
alpha <- y_mean - beta * x_mean

# Weibull parameters:
# Shape parameter (k) is the slope
shape_weibull <- beta

# Scale parameter (lambda) can be derived from intercept
scale_weibull <- exp(-alpha / shape_weibull)

cat("Estimated Weibull Parameters: \n")
cat("Shape (k):", shape_weibull, "\n")
cat("Scale (lambda):", scale_weibull, "\n")

# Plot the fitted Weibull line
lines(X, alpha + beta * X, col = "red")
legend("bottomright", legend = c("Data", "Weibull Fit"), col = c("black", "red"), lty = 1)






#######################################################################################
#                             Practical 10

# The following dataset is collected from a clinical trial in which researchers are testing
# the effectiveness of a new drug compared to a standard drug in increasing the survival
# time of cancer patients. Use a non-parametric method such as the Cox-Mantel test to
# determine if the new drug prolongs survival significantly compared to the standard
# drug.

# New drug  : 10, 22, 12+ , 15+, 17+, 19+, 23+
# Standard drug: 7,11,14,17,18,18,19


lifetimes <- data.frame(
  time = c(10, 22, 12, 15, 17, 19, 23, 7, 11, 14, 17, 18, 18, 19),
  event = c(1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1),
  group = c(rep("A", 7), rep("B", 7))
)

r2 <- sum(lifetimes$group =="B" & lifetimes$event == 1)

df_cen <- lifetimes[lifetimes$event == 1, ]
time_counts <- data.frame(table(df_cen$time))
time_counts$Var1 <- as.numeric(as.character(time_counts$Var1))  

df <- data.frame(time = c(), m = c(), G1 = c(), G2 = c(), R = c(), A = c())


for (i in 1:nrow(time_counts)) {
  time_point <- time_counts$Var1[i]
  
  G1 <- sum(lifetimes$group == "A" & lifetimes$time >= time_point)
  G2 <- sum(lifetimes$group == "B" & lifetimes$time >= time_point)
  
  R <- G1 + G2
  A <- G2 / R  
  
  df_row <- data.frame(
    time = time_point,
    m = time_counts$Freq[i],
    G1 = G1,
    G2 = G2,
    R = R,
    A = A
  )
  df <- rbind(df, df_row)
}
print(df)

U <- r2 - sum(df$m * df$A)

I <- sum(df$m* (df$R - df$m)*df$A*(1-df$A)/(df$R - 1))

Z <- U/sqrt(I)
cat("Cox-Mantel Test Statistic (Z):", Z)


























